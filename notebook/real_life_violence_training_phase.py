# -*- coding: utf-8 -*-
"""real-life-violence-training-phase.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ItyDai0L2FxEpn1C0cZZnDmrVc4o1dAb
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATASETS
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE CELL.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.error import HTTPError
from urllib.parse import unquote
from urllib.request import urlopen
from zipfile import ZipFile

HOME = '/content'
CHUNK_SIZE = 40960
DATASET_MAPPING = 'real-life-violence-situations-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F176381%2F397693%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20230424%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20230424T063507Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6c85cce794b009e994f6eb2204c3e8c7ef1012f9894caec4d337384fcbbda57b0e8c6c08247bb99bf3a0be91aee67220e22a0f924b0a6d6b6d74f8ddfe8e7ede12fbb28de79e92f6d42caac9963ad79e929354f2d83e78243c4173109168a1f2fb76e2aaebefc73fc82bdea1f2b295b1ffdd0a0565b5a8b0632d6f66342229834f766ab121bebd31cfe3caceb1f385333b44b28d6a766b4594a69a834c6982a64c9aa57ad0c67881f28e779007f8c711aa677e6a9be3e7dc6caba09dd768194f87f9705a5ff64b50e286150bcc7234395e90b3b76e08fe4b5787fea963ecc61de921537b080eae6d51b7dffa45ee02b27a9b1701d5426f20366ff338667478a4'
KAGGLE_INPUT_PATH = '/content/input'
KAGGLE_INPUT_SYMLINK = '/kaggle'

if not os.path.exists(KAGGLE_INPUT_PATH):
    os.makedirs(KAGGLE_INPUT_PATH, 777)

# os.symlink(KAGGLE_INPUT_PATH, os.path.join('..', 'input'), target_is_directory=True)

if not os.path.exists(KAGGLE_INPUT_SYMLINK):
    os.makedirs(KAGGLE_INPUT_SYMLINK)

# os.symlink(KAGGLE_INPUT_PATH, os.path.join(KAGGLE_INPUT_SYMLINK, 'input'), target_is_directory=True)

for dataset_mapping in DATASET_MAPPING.split(','):
    directory, download_url_encoded = dataset_mapping.split(':')
    download_url = unquote(download_url_encoded)
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as zipfileres, NamedTemporaryFile() as tfile:
            total_length = zipfileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes zipped')
            dl = 0
            data = zipfileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50 - done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = zipfileres.read(CHUNK_SIZE)
            print(f'\nUnzipping {directory}')
            with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue
print('Dataset import complete.')

"""# **Importing the required libraries**"""

# Commented out IPython magic to ensure Python compatibility.
import cv2
import random
import numpy as np
from numpy.random.mtrand import shuffle
# TF
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

plt.style.use("seaborn-v0_8")
# %matplotlib inline

import imgaug.augmenters as iaa

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from keras.layers import *
from keras.models import Sequential
from tensorflow.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, EarlyStopping, \
    ReduceLROnPlateau, CSVLogger

"""# **Visualize the Data**"""

from IPython.display import HTML
from base64 import b64encode


# To Show a Video in Notebook
def Play_Video(filepath):
    html = ''
    video = open(filepath, 'rb').read()
    src = 'data:video/mp4;base64,' + b64encode(video).decode()
    html += '<video width=640 muted controls autoplay loop><source src="%s" type="video/mp4"></video>' % src
    return HTML(html)


# Classes Directories
NonViolenceVideos_Dir = "input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/"
ViolenceVideos_Dir = "input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/"

# Retrieve the list of all the video files present in the Class Directory.
NonViolence_files_names_list = os.listdir(NonViolenceVideos_Dir)
Violence_files_names_list = os.listdir(ViolenceVideos_Dir)

# Randomly select a video file from the Classes Directory.
# Random_NonViolence_Video = random.choice(NonViolence_files_names_list)
# Random_Violence_Video = random.choice(Violence_files_names_list)

"""**Play Random Non Violence Video**"""

# NonViolence_files_names_list

"""**Play Random Violence Video**"""

# Play_Video(f"{ViolenceVideos_Dir}/{Random_Violence_Video}")

"""# **Extracting Frames**"""

# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT, IMAGE_WIDTH = 224, 224

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 15

DATASET_DIR = "input/real-life-violence-situations-dataset/Real Life Violence Dataset/"

CLASSES_LIST = ["NonViolence", "Violence"]

# @title Extract file name as data and label
np.random.seed(0)

data = []
label = []
for class_index, class_name in enumerate(CLASSES_LIST):

    print(f'Extracting Data of Class: {class_name}')

    # Get the list of video files present in the specific class name directory.
    files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
    shuffle(files_list)
    # Iterate through all the files present in the files list.
    #
    for file_name in files_list:  # specific number video per class

        # Get the complete video path.
        data.append(os.path.join(DATASET_DIR, class_name, file_name))
        label.append(class_name)


# @title Func: frame extraction

def frames_extraction(video_path):
    frames_list = []

    # Read the Video File
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, image = video_reader.read()

        # Apply image augmentation

        # flip = iaa.Fliplr(1.0)
        zoom = iaa.Affine(scale=1.3)
        random_brightness = iaa.Multiply((1, 1.3))
        # rotate = iaa.Affine(rotate=(-25, 25))

        # image_aug = flip(image = image)
        image_aug = random_brightness(image=image)
        image_aug = zoom(image=image_aug)
        # image_aug = rotate(image = image_aug)

        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(image_aug, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame
        normalized_frame = resized_frame / 255

        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)

    video_reader.release()

    return frames_list


"""# **Creating the Dataset**"""


class Dataset:
    def __init__(self, data, label):
        # the paths of images
        self.data = np.array(data)
        # the paths of segmentation images

        # binary encode
        label_encoder = LabelEncoder()
        integer_encoded = label_encoder.fit_transform(label)

        onehot_encoder = OneHotEncoder()
        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)

        self.label = onehot_encoded
        self.w = IMAGE_WIDTH
        self.h = IMAGE_HEIGHT

    def __len__(self):
        return len(self.data)

    def __getitem__(self, i):
        # read data
        frames = frames_extraction(self.data[i])
        label = self.label[i]
        return frames, label


class Dataloader(tf.keras.utils.Sequence):
    def __init__(self, dataset, batch_size, size):
        self.dataset = dataset
        self.batch_size = batch_size
        self.size = size

    def __getitem__(self, i):
        # collect batch data
        start = i * self.batch_size
        stop = (i + 1) * self.batch_size
        data = []
        for j in range(start, stop):
            data.append(self.dataset[j])

        batch = [np.stack(samples, axis=0) for samples in zip(*data)]
        return tuple(batch)

    def __len__(self):
        return self.size // self.batch_size


"""**create dataset function**"""


# @title create dataset func
def create_dataset():
    features = []
    labels = []
    video_files_paths = []

    # Iterating through all the classes.
    for class_index, class_name in enumerate(CLASSES_LIST):

        print(f'Extracting Data of Class: {class_name}')

        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        shuffle(files_list)
        # Iterate through all the files present in the files list.
        # 
        for file_name in files_list[:100]:  # specific number video per class

            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:
                # Append the data to their respective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    features = np.asarray(features)
    labels = np.array(labels)

    return features, labels, video_files_paths


# @title Fit to model
X, X_test, y, y_test = train_test_split(data, label, test_size=0.1, random_state=42)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Build dataset
train_dataset = Dataset(X_train, y_train)
val_dataset = Dataset(X_val, y_val)
test_dataset = Dataset(X_test, y_test)
# print(len(X_train), len(X_val), len(X_test))

"""# **Importing model and Fine-Tuning it.**

# **MobileNet_V2**
"""

from keras.applications.mobilenet_v2 import MobileNetV2

my_model = MobileNetV2(include_top=False, weights="imagenet", input_shape=[224, 224, 3])

# Fine-Tuning to make the last 40 layer trainable
my_model.trainable = False

# my_model.summary()

"""# **Xception**"""

from keras.applications.xception import Xception

my_model = Xception(include_top=False, weights="imagenet", input_shape=[224, 224, 3])

# Fine-Tuning to make the last 40 layer trainable
my_model.trainable = False

# my_model.summary()

"""# **ResNet_V2**"""

from keras.applications.resnet_v2 import ResNet101V2

my_model = ResNet101V2(include_top=False, weights='imagenet')

# Fine-Tuning to make the last 40 layer trainable
my_model.trainable = False

# my_model.summary()

# @title Densenet201
from keras.applications.densenet import DenseNet201

my_model = DenseNet201(include_top=False, weights="imagenet")

my_model.trainable = False

"""# **Building The Model**"""


def create_model():
    # define
    lstm_fw = LSTM(units=32)
    lstm_bw = LSTM(units=32, go_backwards=True)
    optimizers = tf.keras.optimizers

    model = Sequential([
        # Specifying Input to match features shape
        Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)),

        # Passing pre-trained in the TimeDistributed layer to handle the sequence
        TimeDistributed(my_model),
        Dropout(0.25),
        # Time distributed
        TimeDistributed(Flatten()),
        # BiLSTM
        Bidirectional(lstm_fw, backward_layer=lstm_bw),
        Dropout(0.25),  # dropout LSTM
        # Fully connected layer
        # Layer 1
        Dense(256, activation='relu'),
        Dropout(0.25),  # dropout 1
        Dense(128, activation='relu'),
        Dropout(0.25),  # dropout 2
        # Layer 3
        Dense(64, activation='relu'),
        Dropout(0.25),  # dropout 3
        #  Layer 4
        Dense(32, activation='relu'),
        Dropout(0.25),  # dropout 4
        # #Layer output
        # softmax
        Dense(len(CLASSES_LIST), activation='sigmoid')
    ])

    adam = optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
    model.compile(loss="binary_crossentropy", optimizer=adam, metrics=["accuracy"])

    model.summary()

    return model


# Constructing the Model
my_LSTM_model = create_model()

# Plot the structure of the constructed LRCN model.
# plot_model(my_LSTM_model, to_file = 'my_LSTM_model_structure_plot.png', show_shapes = True, show_layer_names = True)

"""# **Specifying Callbacks and Fitting**"""

patience = 3

start_lr = 0.00001
min_lr = 0.00001
max_lr = 0.0001

batch_size = 16

rampup_epochs = 5
sustain_epochs = 2
exp_decay = .8


def lrfn(epoch):
    if epoch < rampup_epochs:
        return (max_lr - start_lr) / rampup_epochs * epoch + start_lr
    elif epoch < rampup_epochs + sustain_epochs:
        return max_lr
    else:
        return (max_lr - min_lr) * exp_decay ** (epoch - rampup_epochs - sustain_epochs) + min_lr


lr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=False)


class myEndCallback(Callback):
    def on_epoch_end(self, epoch, logs={}):
        if ((logs.get('accuracy') >= 0.999)):
            print("\nLimits Reached cancelling training!")
            self.model.stop_training = True


end_callback = myEndCallback()

csv_logger = CSVLogger(HOME + '/training_log.csv')

# Create Early Stopping Callback to monitor the accuracy
early_stopping_callback = EarlyStopping(patience=patience, monitor='val_loss',
                                        mode='min', restore_best_weights=True,
                                        verbose=1, min_delta=.0005)

# Create checkpoint for model
# Change name for model

checkpoint_filepath = 'model/ModelWeights.h5'

model_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,
                                    save_weights_only=True,
                                    monitor='val_loss',
                                    mode='min',
                                    verbose=1,
                                    save_best_only=True)

# Create ReduceLROnPlateau Callback to reduce overfitting by decreasing learning
reduce_lr = ReduceLROnPlateau(monitor='val_loss',
                              factor=0.8,
                              patience=patience,
                              min_lr=0.00001,
                              verbose=1)
callbacks = [end_callback, lr_callback, early_stopping_callback, reduce_lr, csv_logger]

# @title Create loader

# Loader

train_loader = Dataloader(train_dataset, batch_size, len(train_dataset))
val_loader = Dataloader(val_dataset, batch_size, len(val_dataset))
test_loader = Dataloader(test_dataset, batch_size, len(test_dataset))

print(len(train_dataset), len(val_loader), len(test_loader))
# print(features_test.shape, labels_test.shape)

# @title Training with loader

my_LSTM_model_history = my_LSTM_model.fit(train_loader, validation_data=val_loader,
                                          epochs=20,
                                          batch_size=batch_size,
                                          callbacks=callbacks,
                                          use_multiprocessing=True)

# save model
my_LSTM_model.save(HOME + '/resnetModel.h5')
print('Model Saved!')

from google.colab import files

files.download('/content/resnetModel.h5')

files.download('/content/val_los.png')

"""# **Model Evaluation**"""


def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):
    metric_value_1 = model_training_history.history[metric_name_1]
    metric_value_2 = model_training_history.history[metric_name_2]

    # Get the Epochs Count
    epochs = range(len(metric_value_1))

    plt.plot(epochs, metric_value_1, 'blue', label=metric_name_1)
    plt.plot(epochs, metric_value_2, 'orange', label=metric_name_2)

    plt.title(str(plot_name))

    plt.legend()


plot_metric(my_LSTM_model_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
plt.savefig('val_los.png')

plot_metric(my_LSTM_model_history, 'accuracy', 'val_accuracy', 'Total Acc vs Total Validation Accuracy')
plt.savefig('val_acc.png')

"""# **Predicting the Test Set**"""


def create_test_set(test_data):
    features = []
    labels = test_data.label
    for video_file_path in test_data.data:
        # Extract the frames of the video file.
        frames = frames_extraction(video_file_path)
        # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified.
        # So ignore the videos having frames less than the SEQUENCE_LENGTH.
        if len(frames) == SEQUENCE_LENGTH:
            # Append the data to their respective lists.
            features.append(frames)

    features = np.asarray(features)
    return features, labels


features_test, labels_test = create_test_set(test_dataset)

labels_predict = my_LSTM_model.predict(features_test, verbose=False)

# Decoding the data to use in Metrics
labels_predict = np.argmax(labels_predict, axis=1)
labels_test_normal = np.argmax(labels_test, axis=1)

"""# **Accuracy Score**"""

from sklearn.metrics import accuracy_score

AccScore = accuracy_score(labels_predict, labels_test_normal)
print('Accuracy Score is : ', AccScore)

"""Confusion Matrix"""

import seaborn as sns
from sklearn.metrics import confusion_matrix

ax = plt.subplot()
cm = confusion_matrix(labels_test_normal, labels_predict)
sns.heatmap(cm, annot=True, fmt='g', ax=ax)

ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix')
ax.xaxis.set_ticklabels(['NonViolence', 'Violence'])
ax.yaxis.set_ticklabels(['NonViolence', 'Violence'])
plt.savefig('confusion_matrix.png')

"""# **Classification Report**"""

from sklearn.metrics import classification_report

ClassificationReport = classification_report(labels_test_normal, labels_predict)
print('Classification Report is : \n', ClassificationReport)

"""# **Prediction Frame By Frame**"""


def predict_frames(video_file_path, output_file_path, SEQUENCE_LENGTH):
    # Read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # VideoWriter to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('m', 'p', '4', 'v'),
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Store the predicted class in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        ok, frame = video_reader.read()

        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame 
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # We Need at Least number of SEQUENCE_LENGTH Frames to perform a prediction.
        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:
            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = XcepLSTM_model.predict(np.expand_dims(frames_queue, axis=0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        if predicted_class_name == "Violence":
            cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 12)
        else:
            cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 12)

        # Write The frame into the disk using the VideoWriter
        video_writer.write(frame)

    video_reader.release()
    video_writer.release()


plt.style.use("default")


# To show Random Frames from the saved output predicted video (output predicted video doesn't show on the notebook but can be downloaded)
def show_pred_frames(pred_video_path):
    plt.figure(figsize=(20, 15))

    video_reader = cv2.VideoCapture(pred_video_path)

    # Get the number of frames in the video.
    frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Get Random Frames from the video then Sort it
    random_range = sorted(random.sample(range(SEQUENCE_LENGTH, frames_count), 12))

    for counter, random_index in enumerate(random_range, 1):

        plt.subplot(5, 4, counter)

        # Set the current frame position of the video.  
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, random_index)

        ok, frame = video_reader.read()

        if not ok:
            break

        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        plt.imshow(frame)
        ax.figure.set_size_inches(20, 20)
        plt.tight_layout()

    video_reader.release()


# Construct the output video path.
test_videos_directory = 'test_videos'
os.makedirs(test_videos_directory, exist_ok=True)

output_video_file_path = f'{test_videos_directory}/Output-Test-Video.mp4'

# Specifying video to be predicted
input_video_file_path = "../input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/V_670.mp4"

# Perform Prediction on the Test Video.
predict_frames(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Show random frames from the output video
show_pred_frames(output_video_file_path)

# Play the actual video
Play_Video(input_video_file_path)

# Specifying video to be predicted
input_video_file_path = "../input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/NV_102.mp4"

# Perform Prediction on the Test Video.
predict_frames(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Show random frames from the output video
show_pred_frames(output_video_file_path)

# Play the actual video
Play_Video(input_video_file_path)

"""# **Prediction For The Video**"""


def predict_video(video_file_path, SEQUENCE_LENGTH):
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Declare a list to store video frames we will extract.
    frames_list = []

    # Store the predicted class in the video.
    predicted_class_name = ''

    # Get the number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)

    # Iterating the number of times equal to the fixed length of sequence.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        success, frame = video_reader.read()

        if not success:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list
        frames_list.append(normalized_frame)

    # Passing the  pre-processed frames to the model and get the predicted probabilities.
    predicted_labels_probabilities = XcepLSTM_model.predict(np.expand_dims(frames_list, axis=0))[0]

    # Get the index of class with highest probability.
    predicted_label = np.argmax(predicted_labels_probabilities)

    # Get the class name using the retrieved index.
    predicted_class_name = CLASSES_LIST[predicted_label]

    # Display the predicted class along with the prediction confidence.
    print(f'Predicted: {predicted_class_name}\nConfidence: {predicted_labels_probabilities[predicted_label]}')

    video_reader.release()


# Specifying video to be predicted
input_video_file_path = "../input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/V_276.mp4"

# Perform Single Prediction on the Test Video.
predict_video(input_video_file_path, SEQUENCE_LENGTH)

# Play the actual video
Play_Video(input_video_file_path)

# Specifying video to be predicted
input_video_file_path = "../input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/NV_23.mp4"

# Perform Single Prediction on the Test Video.
predict_video(input_video_file_path, SEQUENCE_LENGTH)

# Play the actual video
Play_Video(input_video_file_path)
